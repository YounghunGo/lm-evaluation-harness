[2023-09-19 03:14:05,234] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 03:14:07,047] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-19 03:14:07,263] [INFO] [runner.py:570:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --model hf-causal-experimental --model_args pretrained=/home/work/llama-30b-hf,use_accelerate=False --batch_size 8 --tasks hellaswag --no_cache --device cuda
[2023-09-19 03:14:09,100] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 03:14:10,709] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.8.2
[2023-09-19 03:14:10,709] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-09-19 03:14:10,709] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-09-19 03:14:10,709] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-09-19 03:14:10,709] [INFO] [launch.py:163:main] dist_world_size=4
[2023-09-19 03:14:10,709] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2023-09-19 03:14:13,879] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 03:14:13,879] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 03:14:13,889] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-19 03:14:13,897] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Selected Tasks: ['hellaswag']
start timestamping
Selected Tasks: ['hellaswag']
start timestamping
Selected Tasks: ['hellaswag']
start timestamping
Selected Tasks: ['hellaswag']
start timestamping
pretrained load time:  264.81258 sec
[2023-09-19 03:18:43,156] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-19 03:18:43,159] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
_create_model_parallel_group
[2023-09-19 03:18:43,159] [INFO] [comm.py:637:init_distributed] cdb=None
pretrained load time:  264.81315 sec
[2023-09-19 03:18:43,161] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
pretrained load time:  264.83688 sec
[2023-09-19 03:18:43,163] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-19 03:18:43,164] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
_create_model_parallel_group
[2023-09-19 03:18:43,165] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-09-19 03:18:43,165] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-19 03:18:43,166] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
_create_model_parallel_group
[2023-09-19 03:18:43,166] [INFO] [comm.py:637:init_distributed] cdb=None
pretrained load time:  264.86638 sec
[2023-09-19 03:18:43,174] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.2, git-hash=unknown, git-branch=unknown
[2023-09-19 03:18:43,175] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
_create_model_parallel_group
[2023-09-19 03:18:43,176] [INFO] [comm.py:637:init_distributed] cdb=None
local_rank 1
local_rank 0
local_rank 3
AutoTP:  [(<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>, ['mlp.down_proj', 'self_attn.o_proj'])]
local_rank 2
AutoTP:  [(<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>, ['mlp.down_proj', 'self_attn.o_proj'])]
AutoTP:  [(<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>, ['self_attn.o_proj', 'mlp.down_proj'])]
AutoTP:  [(<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>, ['mlp.down_proj', 'self_attn.o_proj'])]
deepspeed load time:  24.62464 secdeepspeed load time:  24.61189 sec

deepspeed load time:  24.62307 secdeepspeed load time:  24.63031 sec

 290.01823 sec
null
 290.02312 sec
null
 290.11791 sec
null
 290.14987 sec
null
[2023-09-19 03:19:10,068] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2615812
[2023-09-19 03:19:10,373] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2615813
[2023-09-19 03:19:10,374] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2615814
[2023-09-19 03:19:10,472] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2615815
[2023-09-19 03:19:10,473] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', 'main.py', '--local_rank=3', '--model', 'hf-causal-experimental', '--model_args', 'pretrained=/home/work/llama-30b-hf,use_accelerate=False', '--batch_size', '8', '--tasks', 'hellaswag', '--no_cache', '--device', 'cuda'] exits with return code = 1
